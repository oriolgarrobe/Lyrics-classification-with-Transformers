# Comparison of Transformer and non-Transformer Models for a Text Classification Task

Click here to access the full report [:page_with_curl:](https://github.com/oriolgarrobe/Text-classification/blob/main/TM_Project_Garrobe.pdf)

**Abstract**

The Natural Language Processing (NLP) field is changing with the introduction of the Transformer in 2017. The Transformer has replaced many Machine Learning (ML)
methods achieving state-of-the-art results. However, in some particular cases such complex and computationally expensive models achieve results very similar to other, simpler, models. From this regards, 3 different ML approaches are trained on a dataset that provides information of Headlines and short descriptions of news articles from the Huffington Post and classifies them between different categories, such as Politics or Entertainment, among others. In this scenario non-Transformer models achieve similar results to the Transformer models for large amounts of data while needing much less training time. On the other hand, Transformer models achieve substantially better results for more difficult tasks and small amounts of data. All this results are thoroughly analysed, providing a clear image of when to use a Transformer-based model.

**Dataset** 

* The data was downlowaded from [Kaggle](https://www.kaggle.com/datasets) and it is called [News Category Dataset](https://www.kaggle.com/rmisra/news-category-dataset) 
* The dataset used for this project contains 202,372 records from the year 2012 to 2018 obtained from the [HuffPost](https://www.huffpost.com/)


